{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tqdm as notebook_tqdm\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import json\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128000\n",
    "\n",
    "\n",
    "def load_lama31(\n",
    "    model_name: str = \"unsloth/Meta-Llama-3.1-8B-Instruct\", max_seq_length: int = 128000\n",
    "):\n",
    "    # Loading LLaMA with a 128k context window\n",
    "    if (\n",
    "        os.path.exists(os.path.join(model_name, \"tokenizer_config.json\"))\n",
    "        and os.path.exists(os.path.join(model_name, \"tokenizer.json\"))\n",
    "        and os.path.exists(os.path.join(model_name, \"special_tokens_map.json\"))\n",
    "    ):\n",
    "\n",
    "        model_name_1 = model_name\n",
    "        print(f\"Modelfound at repo {model_name}\")\n",
    "\n",
    "    else:\n",
    "        model_name_1 = \"unsloth/Meta-Llama-3.1-8B-Instruct\"\n",
    "        print(\n",
    "            f\"Model not found at repo {model_name}. Loading default model {model_name_1}\"\n",
    "        )\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name_1,\n",
    "        max_seq_length=max_seq_length,  # Set context window size to 128k tokens\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,  # To reduce memory usage\n",
    "    )\n",
    "\n",
    "    print(\"Model loaded to VRAM Sucessfully\")\n",
    "    model = peft_model(model)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def tokenize_data(texts, tokenizer, max_length=max_seq_length):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "\n",
    "def peft_model(model):\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",  # Optimized memory usage\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsloth_chat_inf(model, tokenizer):\n",
    "    tokenizer1 = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template=\"llama-3.1\",\n",
    "    )\n",
    "    model = FastLanguageModel.for_inference(model)\n",
    "    return model, tokenizer1\n",
    "\n",
    "\n",
    "# print(\"sam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/media/qult/volume/Taimour/Github Repos/Entity-Extraction-Fine-Tuning/Llama Inference/model\"\n",
    "\n",
    "model, token = load_lama31(model_path, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    token,\n",
    "    chat_template=\"llama-3.1\",\n",
    "    mapping={\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\",\n",
    "    },  # ShareGPT style\n",
    ")\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_PATH = \"/media/qult/volume/Taimour/Real Time Machine Telemetry Dashboard/data/aggreate_metrics.json\"\n",
    "\n",
    "with open(JSON_PATH, \"r\") as file:\n",
    "    input_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityname = \"Karachi\"\n",
    "\n",
    "prompt_general = f\"\"\"\n",
    "You are an advanced data analysis assistant specialized in weather and climate data. Your task is to analyze detailed weather data for various cities and provide structured, insightful responses tailored to specific user queries. Using the provided dataset as context, generate insights about the weather in a city of interest. In this case, the focus is on {cityname}.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the prompt with the user data\n",
    "prompt = str(input_data) + prompt_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to extract entities from the raw data provided by the user in the json format and provide the extracted entities in the specified format as described in the prompt.\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": prompt},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "results = model.generate(\n",
    "    input_ids=inputs, streamer=text_streamer, max_new_tokens=10000, use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the generated text and remove unnecessary tokens\n",
    "response = tokenizer.decode(results[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the exact response\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
